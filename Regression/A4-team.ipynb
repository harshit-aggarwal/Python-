{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz5o-GqmGmTI"
   },
   "source": [
    "**Part A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "139997b1"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import zscore\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3b209fb",
    "outputId": "0945d016-0af6-4281-997d-159563edb12a"
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "a = pd.read_csv(\"Amazon.csv\").drop(['Sr.No'], axis=1)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82b28080",
    "outputId": "8a2049ec-4520-425c-ed70-8ab58008cc0b"
   },
   "outputs": [],
   "source": [
    "# Task 1: which social media to focus for a maximized profit\n",
    "\n",
    "# quick view on correlations\n",
    "sns.pairplot(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17d3fc52"
   },
   "source": [
    "Observations: Youtube and Facebook have stronger correlations to Expected profit compared to Instagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2df4806b",
    "outputId": "24289ab8-7d9e-4a60-de5c-c4c1395dd1af"
   },
   "outputs": [],
   "source": [
    "# print and check correlations\n",
    "print(a.corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "035ee46d"
   },
   "source": [
    "Observations:\n",
    "Among independent variables, higher correlation (0.354104) was observed between Instagram and Facebook.\n",
    "Instagram, the social media with lowest contribution to the expected profit has been excluded from the analysis to prevent multicollinearity.\n",
    "    \n",
    "We built several multiple linear regression models with Youtube and Facebook as independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eba4529c"
   },
   "outputs": [],
   "source": [
    "# assign X\n",
    "amazon_data = a[[\"Youtube\", \"Facebook\"]].values\n",
    "\n",
    "# create constant for SM OLS package\n",
    "amazon_data_w_constant = sm.add_constant(amazon_data)\n",
    "\n",
    "# assign y\n",
    "amazon_profit = a[[\"Expected profit \"]].values\n",
    "\n",
    "# name X features\n",
    "amazon_data_names = [\"constant\", \"Youtube\", \"Facebook\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f5dc568"
   },
   "source": [
    "### model 1: Statsmodel OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bb60b41",
    "outputId": "aa8f7310-d6ce-4a71-fff4-4df1cd71c717"
   },
   "outputs": [],
   "source": [
    "# split and fit the model\n",
    "X, y = amazon_data_w_constant, amazon_profit\n",
    "train_X, test_X, train_y, test_y = train_test_split(X , y, train_size = 0.8)\n",
    "\n",
    "OLSMod = sm.OLS(train_y, train_X).fit()\n",
    "print(OLSMod.summary(xname=amazon_data_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9127544",
    "outputId": "2351b07b-1f09-4ba8-e6a7-a4c4ab7d0f46"
   },
   "outputs": [],
   "source": [
    "# print model evaluations for predictions\n",
    "\n",
    "predictions = OLSMod.predict(test_X)\n",
    "\n",
    "R2 = \"{:.2%}\".format(r2_score(test_y, predictions))\n",
    "print(\"Prediction R2:\", R2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "de79a42b",
    "outputId": "f73dc567-2783-4011-8f41-88e194a0092c"
   },
   "outputs": [],
   "source": [
    "# render Residual Analysis\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 7))\n",
    "sm.ProbPlot(OLSMod.resid, fit = True).ppplot(line='45', ax=ax[1]);\n",
    "histplot = sns.histplot(OLSMod.resid,kde=True, color ='blue',ax=ax[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84a6b3df",
    "outputId": "9609c09d-add3-4851-eeee-f2dfb756c9fb"
   },
   "outputs": [],
   "source": [
    "# Find Z-Score for Outlier Analysis\n",
    "a['z_score_price'] = zscore(a[\"Expected profit \"])\n",
    "\n",
    "# Outlier would lie in plus or minus 3 SD.\n",
    "ao = a[(a.z_score_price > 3.0) | (a.z_score_price < -3.0) ]\n",
    "\n",
    "print(ao)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6862a2a0"
   },
   "source": [
    "Observations: No outliers in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aee0fa4"
   },
   "source": [
    "### Model 2: Sklearn OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b37767ef",
    "outputId": "e900c768-f904-4520-ea4d-5bc1bbc5aa8a"
   },
   "outputs": [],
   "source": [
    "# split and fit the model\n",
    "X, y = amazon_data, amazon_profit\n",
    "train_X, test_X, train_y, test_y = train_test_split(X , y, train_size = 0.8)\n",
    "\n",
    "LinReg = LinearRegression(normalize=True, fit_intercept=True)\n",
    "LinReg.fit(train_X, train_y)\n",
    "print(\"Model Score \", LinReg.score(train_X, train_y), \"\\ncoefficients: \", LinReg.coef_, \"\\nintercept: \", LinReg.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "919603c2",
    "outputId": "0f1b38f6-9ca9-4f74-fd3f-90629d2c390e"
   },
   "outputs": [],
   "source": [
    "# print evaluations for predictions\n",
    "predictions = LinReg.predict(test_X)\n",
    "\n",
    "R2 = \"{:.2%}\".format(r2_score(test_y, predictions))\n",
    "\n",
    "print(\"Prediction R2:\", R2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8866516d"
   },
   "source": [
    "### Model 3: ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67fba195",
    "outputId": "ae9ea67d-cd89-4e7b-fa6b-58517eaad2be"
   },
   "outputs": [],
   "source": [
    "# split and fit the model\n",
    "X, y = amazon_data, amazon_profit\n",
    "train_X, test_X, train_y, test_y = train_test_split(X , y, train_size = 0.8)\n",
    "\n",
    "regr = ElasticNet(random_state=0)\n",
    "\n",
    "ElasticNet(random_state=0)\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X , y, train_size = 0.8)\n",
    "\n",
    "regr.fit(train_X, train_y)\n",
    "\n",
    "print(\"Model Score \", regr.score(train_X, train_y), \"\\ncoefficients: \", regr.coef_, \"\\nintercept: \", regr.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd43174a",
    "outputId": "18046907-dfa7-4922-8a06-61d75eea8b3c"
   },
   "outputs": [],
   "source": [
    "# print evaluations for predictions\n",
    "predictions = regr.predict(test_X)\n",
    "\n",
    "R2 = \"{:.2%}\".format(r2_score(test_y, predictions))\n",
    "\n",
    "print(\"Prediction R2:\", R2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9ba933c"
   },
   "source": [
    "Conclusion: Instagram not only has high correlations with Facebook which can cause multicollinearity but also negligible correlations to the profit compared to other two variables. After removing Instagram, we obtained much reliable prediction models. All the three models showed high prediction scores up to 92.84%, which can differ by random splitting. Therefore, these models can provide insights to promote better marketing strategies using YouTube and Facebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1VTeDlXJyAk"
   },
   "source": [
    "**Part B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6tzkiroJv4s"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "wM4MHxFBJwDA",
    "outputId": "ef4cee46-8995-43ac-9198-04eebc89ac6c"
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "data_b0=pd.read_csv('REMAX.csv')\n",
    "data_b0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdYbCDhXUD9f",
    "outputId": "c416373a-ba24-46bd-cba9-aedfc9bb354e"
   },
   "outputs": [],
   "source": [
    "# explore the data\n",
    "data_b0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "iy6anVBzUOMm",
    "outputId": "b75e97aa-0133-4b97-b271-dd8923eb6925"
   },
   "outputs": [],
   "source": [
    "data_b0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvcUdNmpJwI6"
   },
   "outputs": [],
   "source": [
    "# Convert 'Yes', 'No' values to 1 and 0.\n",
    "data_b0['mainroad']=data_b0['mainroad'].map({'yes':1,'no':0})\n",
    "data_b0['guestroom']=data_b0['guestroom'].map({'yes':1,'no':0})\n",
    "data_b0['basement']=data_b0['basement'].map({'yes':1,'no':0})\n",
    "data_b0['hotwaterheating']=data_b0['hotwaterheating'].map({'yes':1,'no':0})\n",
    "data_b0['airconditioning']=data_b0['airconditioning'].map({'yes':1,'no':0})\n",
    "data_b0['prefarea']=data_b0['prefarea'].map({'yes':1,'no':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "ZAk1vGSSJwLe",
    "outputId": "61794718-2637-4ba3-93d5-8db9fcbd1b8f"
   },
   "outputs": [],
   "source": [
    "data_b = pd.get_dummies(data_b0, columns=['furnishingstatus'])\n",
    "data_b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRe5BWDwKFEd"
   },
   "outputs": [],
   "source": [
    "# Assign columns as dependent variable and independent variables\n",
    "## Independent Variable\n",
    "x_b = data_b.drop('price',axis=1)\n",
    "## Dependent Variable\n",
    "y_b = data_b['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wq-8mH6bKFHD"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x_b, test_x_b, train_y_b, test_y_b = train_test_split( x_b_constant,y_b,train_size = 0.8,random_state = 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPgBv_HKKFLk"
   },
   "outputs": [],
   "source": [
    "# Model fit\n",
    "remax_lm=sm.OLS(train_y_b,train_x_b.astype(float)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNm_Mei9KFN0",
    "outputId": "10ead8d5-baa9-4b12-c0d6-5a099b1a1f9e"
   },
   "outputs": [],
   "source": [
    "# Check the estimated parameter results\n",
    "print(remax_lm.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-BymzguvKFQM",
    "outputId": "304b934c-1222-4764-ca46-cfd73bf9bd1d"
   },
   "outputs": [],
   "source": [
    "# Explore stastistical summary\n",
    "print(remax_lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "c7p8LMhSKFUv",
    "outputId": "ee013371-94f7-4d04-b873-118cc17c73cd"
   },
   "outputs": [],
   "source": [
    "# Conduct a residual analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "remax_resid = remax_lm.resid\n",
    "probplot = sm.ProbPlot( remax_resid, fit = True )\n",
    "plt.figure( figsize = (8, 6) )\n",
    "probplot.ppplot( line='45' )\n",
    "plt.title( \"[Fig B-1] - Normal P-P Plot of Regression Standardized Residuals for First Regression\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "muDyc7yaWPXg",
    "outputId": "1785350f-78eb-4e82-e1fd-7f40f2d58b48"
   },
   "outputs": [],
   "source": [
    "# print correlation matrix heatmap\n",
    "corr = data_b.corr()\n",
    "_ = sns.heatmap(corr,\n",
    "                xticklabels=corr.columns.values,\n",
    "                yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxSYjjFgLQfs"
   },
   "outputs": [],
   "source": [
    "# Detect multicollinearity with VIF and model with best fit\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "def get_vif_factors( x_b ):\n",
    "    x_b_matrix = x_b.to_numpy()\n",
    "    vif = [ variance_inflation_factor( x_b_matrix, i ) for i in range( x_b_matrix.shape[1] ) ]\n",
    "    vif_factors = pd.DataFrame()\n",
    "    vif_factors['column'] = x_b.columns\n",
    "    vif_factors['vif'] = vif\n",
    "    return vif_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "73KSC7k4LQjf",
    "outputId": "fd2567fd-d973-4dc7-8bdf-a0254cf69051"
   },
   "outputs": [],
   "source": [
    "vif_factors = get_vif_factors(x_b).sort_values('vif', ascending=False)\n",
    "vif_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "eSgpn24fLQln",
    "outputId": "c4169395-0d60-4196-99ad-409c6feab3b7"
   },
   "outputs": [],
   "source": [
    "# Remove variables with high VIF\n",
    "rmv = ['furnishingstatus_furnished','furnishingstatus_semi-furnished','furnishingstatus_unfurnished']\n",
    "x_b2 = list( set(x_b) - set(rmv) )\n",
    "get_vif_factors(x_b[x_b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KymiEJ3BLQpr",
    "outputId": "bc1ed9e2-a962-4a41-e86a-33ba84fb4677"
   },
   "outputs": [],
   "source": [
    "# Build new model after removing variables with high VIFs\n",
    "train_x_b = train_x_b[x_b2]\n",
    "remax_lm2 = sm.OLS(train_y_b, train_x_b).fit()\n",
    "print(remax_lm2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbM9MBoEJwRF"
   },
   "outputs": [],
   "source": [
    "# Re-do the residual analysis and compare with the first one\n",
    "remax_resid2 = remax_lm2.resid\n",
    "probplot = sm.ProbPlot( remax_resid2, fit = True )\n",
    "plt.figure( figsize = (8, 6) )\n",
    "probplot.ppplot( line='45' )\n",
    "plt.title( \"[Fig B-2] Normal P-P Plot of Regression Standardized Residuals for Second Regression\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B2Ordd6NHT4"
   },
   "source": [
    "**Part C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "758gHsOwNXoY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Read the excel dataset and convert into csv file\n",
    "\n",
    "read = pd.read_excel (\"Inventory.xlsx\")\n",
    "read.to_csv (\"Inventory.csv\",index = None,header=True)\n",
    "data_c=pd.read_csv('Inventory.csv')\n",
    "data_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces from columns to prevent errors\n",
    "data_c.columns = data_c.columns.str.replace(' ', '')\n",
    "data_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset\n",
    "data_c.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw scatter plot to check the relationship between the two variables\n",
    "plt.scatter(data_c['Quantity'],data_c['Cost'])\n",
    "plt.xlabel(\"Quantity\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign dependent and independent variables\n",
    "## Independent Variables\n",
    "x_c = data_c['Quantity']\n",
    "\n",
    "## Dependent Variable\n",
    "y_c = data_c['Cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x_c, test_x_c, train_y_c, test_y_c = train_test_split( x_c,y_c,train_size = 0.8,random_state = 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive the regression model\n",
    "rue_lm=sm.OLS(train_y_c,train_x_c).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the estimated parameter results\n",
    "print(rue_lm.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model diagnostics\n",
    "print(rue_lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rue_resid = rue_lm.resid\n",
    "probplot = sm.ProbPlot( rue_resid, fit = True )\n",
    "plt.figure( figsize = (8, 6) )\n",
    "probplot.ppplot( line='45' )\n",
    "plt.title( \"[Fig C] - Normal P-P Plot of Regression Standardized Residuals\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the z-score for outlier analysis\n",
    "from scipy.stats import zscore\n",
    "data_c['z_score_cost']=zscore(data_c.Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c[(data_c.z_score_cost>3.0) | (data_c.z_score_cost<-3.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same on test dataset\n",
    "pred_y_c = rue_lm.predict( test_x_c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(test_y_c, pred_y_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "\n",
    "# Predict the y values\n",
    "pred_y_c = rue_lm.predict(test_x_c)\n",
    "_, pred_y_low, pred_y_high = wls_prediction_std( rue_lm, alpha = 0.1)\n",
    "pred_y_df = pd.DataFrame( { 'Quantity': test_x_c,\n",
    "'pred_y': pred_y_c,\n",
    "'pred_y_left': pred_y_low,\n",
    "'pred_y_right': pred_y_high } )\n",
    "\n",
    "pred_y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2zEZjQ9YwTO"
   },
   "source": [
    "**Part D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGHGXgy3YtnZ"
   },
   "outputs": [],
   "source": [
    "# Task 1\n",
    "\n",
    "# Import regex library\n",
    "import re\n",
    "\n",
    "# Open both files\n",
    "file1 = open(\"script01.txt\",\"r\") #open script01.txt and save into file1\n",
    "file2 = open(\"script02.txt\", \"r\") #open script02.txt and save into file2\n",
    "\n",
    "script1 = file1.read() #read in file1 into script1\n",
    "script2 = file2.read() #read in file2 into script2\n",
    "\n",
    "script = script1 + script2 #combine script1 and script2\n",
    "\n",
    "# Step 2: Create list and dictionary\n",
    "atoz = \"abcdefghijklmnopqrstuvwxyz\" #create a list of the alphabet\n",
    "\n",
    "outcome = {} #Create empty dictionary\n",
    "\n",
    "# Step 3: Create your own function \n",
    "for character in script: \n",
    "    ch = character.lower() #make all characters lowercase\n",
    "    if ch in atoz: #check to see if the character is in the alphabet\n",
    "        if ch not in outcome: #check to see if character is not already saved in dictionary\n",
    "            outcome[ch] = 1 #Save character in dictionary and set value to 1\n",
    "        else: #Character already saved in dictionary\n",
    "            outcome[ch] += 1 #Add 1 to the value\n",
    "\n",
    "# Sort dictionary from highest to lowest value\n",
    "outcome_sort = sorted(outcome.items(), reverse = True, key=lambda x: x[1])\n",
    "\n",
    "# Display the sorted output \n",
    "outcome_sort\n",
    "\n",
    "# Write the output to text file called parta\n",
    "with open('parta.txt','w') as f:\n",
    "    print(outcome_sort,file=f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0ebEFuyZ4Ab",
    "outputId": "a821f73e-cbb6-4af0-a409-ba211da26797"
   },
   "outputs": [],
   "source": [
    "# Task 2\n",
    "\n",
    "# Import regex library\n",
    "import re\n",
    "\n",
    "file1 = open(\"script01.txt\",\"r\") #open script01.txt and save into file1\n",
    "file2 = open(\"script02.txt\", \"r\") #open script02.txt and save into file2\n",
    "\n",
    "script1 = file1.read() #read in file1 into script1\n",
    "script2 = file2.read() #read in file2 into script2\n",
    "\n",
    "# Combine script1 and script2\n",
    "script = script1 + script2 \n",
    "\n",
    "# Convert all text to lower\n",
    "script = script.lower()\n",
    "\n",
    "# Substitute all characters except alphabets and spaces by an empty space \n",
    "script = re.sub('[^a-z ]+', ' ', script)\n",
    "\n",
    "# Split the string into an array containing individual words\n",
    "words = script.split()\n",
    "\n",
    "# Create empty dictionary\n",
    "outcome2 = {} \n",
    "\n",
    "# For loop to count the occurence of each word\n",
    "for word in words: \n",
    "  if word not in outcome2: # If word is not in outcome\n",
    "    outcome2[word] = 1 # Add the word and set index to 1\n",
    "  else:\n",
    "    outcome2[word] += 1 # Otherwise increase the index by 1\n",
    "\n",
    "# Sort the dict by the number of occurences of each word\n",
    "outcome2_sort = sorted(outcome2.items(), reverse = True, key=lambda x: x[1])\n",
    "\n",
    "# Display the top 10 most frequently occuring words\n",
    "list(outcome2_sort)[:10]\n",
    "\n",
    "# Write the output to text file called partb\n",
    "with open('partb.txt','w') as f:\n",
    "    print(outcome2_sort,file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ad7DXSgilFcS"
   },
   "outputs": [],
   "source": [
    "# Task 3\n",
    "\n",
    "# Get relevant libraries \n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Open script01.txt and save into file1\n",
    "file1 = open(\"script01.txt\",\"r\") \n",
    "\n",
    "# Open script02.txt and save into file1\n",
    "file2 = open(\"script02.txt\",\"r\")\n",
    "\n",
    "# Read in file1 into script1\n",
    "script1 = file1.read() \n",
    "\n",
    "# Read in file2 into script2\n",
    "script2 = file2.read()\n",
    "\n",
    "# Load the data and convert the stop words into a list\n",
    "data = pd.read_csv('stopwords.csv')\n",
    "stop_words = data['above'].tolist()\n",
    "\n",
    "# Convert script1 to lowercase and substitute all characters except alphabets and spaces by an empty space \n",
    "script1 = script1.lower()\n",
    "script1 = re.sub('[^a-z ]+', ' ', script1)\n",
    "\n",
    "# Split script1 into a list of words\n",
    "script1 = script1.split()\n",
    "\n",
    "# Convert script2 to lowercase and substitute all characters except alphabets and spaces by an empty space \n",
    "script2 = script2.lower()\n",
    "script2 = re.sub('[^a-z ]+', ' ', script2)\n",
    "\n",
    "# Split script2 into a list of words\n",
    "script2 = script2.split()\n",
    "\n",
    "# Create empty list to hold all the filtered words from script1\n",
    "final_words1 = []\n",
    "\n",
    "# Check all words in script1\n",
    "for word in script1:\n",
    "    if word not in stop_words: # If word is not in stop words\n",
    "        if len(word) >= 2: # If the word is not a singleton\n",
    "            final_words1.append(word)  # Add the word to list holding the filtered words from script1\n",
    "\n",
    "# Create empty list to hold all the filtered words from script2\n",
    "final_words2 = []\n",
    "\n",
    "# Check all words in script1\n",
    "for word in script2:\n",
    "    if word not in stop_words: # If word is not in stop words\n",
    "        if len(word) >= 2: # If the word is not a singleton\n",
    "            final_words2.append(word)  # Add the word to list holding the filtered words from script2               \n",
    "\n",
    "outcome_script1 = {} # Create empty dictionary to hold word count\n",
    "\n",
    "# For all filtered words from script1\n",
    "for word in final_words1: \n",
    "    \n",
    "  if word not in outcome_script1: # Check to see if word is not already saved in dictionary\n",
    "    outcome_script1[word] = 1 # Save word in dictionary and set value to 1\n",
    "  else:  # Word already saved in dictionary\n",
    "    outcome_script1[word] += 1 # Increase the count occurrence of the word\n",
    "\n",
    "# Sort dictionary from highest to lowest value\n",
    "outcome_script1 = sorted(outcome_script1.items(), reverse = True, key=lambda x: x[1]) \n",
    "\n",
    "# Grab the top 10 values which represent the 10 most occurring words and their counts\n",
    "top_10 = (outcome_script1)[:10] \n",
    "\n",
    "# Empty list to store the top 10 words\n",
    "top_10_words = []\n",
    "\n",
    "# Grab the 10 most occurring words from the list, which is the first part of a tuple\n",
    "for word in top_10:\n",
    "    top_10_words.append(word[0])\n",
    "\n",
    "# Empty dict to store counts of occurring in script2\n",
    "script2_count = {}\n",
    "\n",
    "# Check all filtered words in script2\n",
    "for word in final_words2:\n",
    "    if word in top_10_words: # If the word exists in the list containing top 10 words from script1\n",
    "        if word not in script2_count: # Check to see if word already saved in the dict\n",
    "            script2_count[word] = 1 # Save word in dict and set value to 1\n",
    "        else: # Word already saved in dict\n",
    "            script2_count[word] += 1 # Increase the count occurrence of the word\n",
    "\n",
    "# Check all filtered words in script2\n",
    "for word in top_10_words:\n",
    "    if word not in script2_count: # If word did not appear in top 10 list\n",
    "        script2_count[word] = 0 # Set the count of that word to 0\n",
    "\n",
    "# Sory both lists containing the words and their occurrence count alphabetically\n",
    "count = sorted(script2_count.items(), reverse = False, key=lambda x: x[0])\n",
    "top_10 = sorted(top_10, reverse = False, key=lambda x: x[0])\n",
    "\n",
    "# Convert the lists to dataframes\n",
    "df1 = pd.DataFrame(top_10, columns =['Word', 'Script1_count'])\n",
    "df2 = pd.DataFrame(count, columns =['Word', 'Script2_count'])\n",
    "\n",
    "# Merge the 2 dataframes to get the word and thier counts in both scripts\n",
    "data = df1.merge(df2)\n",
    "\n",
    "# Print the resulting dataframe\n",
    "data\n",
    "\n",
    "# Write the output to text file called partc\n",
    "with open('partc.txt','w') as f:\n",
    "    print(data,file=f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
